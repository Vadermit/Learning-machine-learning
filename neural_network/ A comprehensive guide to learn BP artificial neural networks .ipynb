{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A comprehensive guide to learn BP artificial neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> About the author: Jinming Yang(杨津铭), an undergraduate school student in Sun Yat-Sen University who focuses on machine learning and transportation research. Email: mountty@qq.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will give a comprehensive introduction to the math and mechanism behind neural networks through a fully self-coded python demo. You will learn what an artificial neural network is, what is the mechanism behind it and how to easily implement a neural network through a mighty tool for ML & DL- Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Github: https://github.com/Vadermit/Learning-machine-learning/tree/master/neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Artificial Neural Networks?\n",
    "\n",
    "Artificial Neural Networks are structured and integrated processors which contians multiple parameters and operations. Like an ordinary function, you feed in inputs, the neural networks will give you outputs in return. The difference is, you don't have to manually design the functions bit by bit in neural networks, you don't have to set the complicated parameters and operations based on physical model you want to describe. Instead, you can use the sofisticated structure - Artificial Neural Networks, and you feed in labeled datas(I will talk about it later) to train(adjust) the parameters in the neural network and then the neural network may better fit the inputs and outputs than the functions you take days to derive manually based on its physical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a three layer neural network with three neurons in each of the first two layers looks like:\n",
    "<img src=\"images/3layer_nn.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of artificial neurons\n",
    "### Neurons in the input layer\n",
    "There is nothing fancy thing in the neuron of the input layer. It doesn't have the sum nor the activation operation as neurons in other layers do. Here is a structure demonstration of the first input neuron in the example artificial neural network above.\n",
    "\n",
    "<img src=\"images/input_neuron.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "You can see that the input remains unchanged after it went through the input neuron.\n",
    "\n",
    "### Neurons in the hidden layer\n",
    "In the hidden layer, things get a little bit different. Neurons in hidden layers was feed in weighted outputs from higher layer. Neurons in hidden layers will sum the inputs up, then pass the summation to an activation function which we will not dig deep here. You can check the blog written by Avinash: https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0. Here comes another structure demonstration of a neuron, but this time, the neuron comes from the hidden layer.\n",
    "\n",
    "<img src=\"images/hidden_layer_neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The inputs went through three operations in the hidden layer: multiplied bu a weight, sum the weighted inputs up and pass through the activation function.\n",
    "\n",
    "### Neurons in the output layer\n",
    "There is not much difference between neurons in hidden layer and the output layer. Here is a demonstration.\n",
    "\n",
    "<img src=\"images/output_neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Bias\n",
    "Now some of you might ask what is the grey <span style=\"color:blue\">$b$</span> in the ANN structure above stands for. Well actually, the <span style=\"color:blue\">$b$</span>s above are not neurons. They are just biases which are fed into neurons in the next layer as the outputs of the current layer do. Let's look at the operations in a hidden layer neuron in math, and you will understand what a bias do.\n",
    "\n",
    "$$h_1 = f(w_{11}^{(1)}x_1+w_{21}^{(1)}x_2+w_{31}^{(1)}x_3+\\color{red}{b_1^{(1)}})$$\n",
    "\n",
    "Where <span style=\"color:blue\">$f(\\bullet)$</span> is the activation funtion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "Now we can see, the artificial neurons mainly do two things: calculating the weighted summation of inputs coming from higher layer, then feed it in the activation function. Then what is activation function? what does it do?\n",
    "\n",
    "Well basically, you can imaging the activation function as a gate, it decides whether and how much to let the sum pass through the neuron. So, the activation function decides whether and how much a neuron would affect deeper layers. And mathematically the activation function increased the nonlinearity of the neural network so it can fit some nonlinear relations.\n",
    "\n",
    "There are many kinds of activation functions, and here I'll only introduce 3 of them: sigmoid, tanh and ReLU. The detail of activation function can be seen on https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0.\n",
    "\n",
    "### Sigmoid\n",
    "The sigmoid activation function looks like this:\n",
    "$$f(z) = \\frac{1}{1+e^{-z}}$$\n",
    "<img src=\"images/sigmoid.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Its derivative is:\n",
    "$$\\frac{df(z)}{dz} = f(z)(1 - f(z))$$\n",
    "\n",
    "### Tanh\n",
    "The tanh activation function looks like this:\n",
    "$$f(z) = \\frac{2}{1+e^{-2z}} - 1$$\n",
    "<img src=\"images/tanh.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Its derivative is:\n",
    "$$\\frac{df(z)}{dz} = 1 - f(z)^2$$\n",
    "\n",
    "### ReLU\n",
    "The ReLU activation function looks like this:\n",
    "$$f(z) = max(0, z)$$\n",
    "<img src=\"images/ReLU.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Its derivative is:\n",
    "$$\\frac{df(z)}{dz} = \\left\\{\n",
    "\\begin{aligned}\n",
    "0, & & z < 0\\\\\n",
    "1, & & z > 0 \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Process\n",
    "The feedforward process of the artificial neural network mentioned before can be represented by a python code as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import some supporting function library:\n",
    "* numpy: providing data structure and matrix calculation support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function\n",
    "We introduce three kinds of activation functions(i.e. sigmoid, tanh, ReLU) plus one no activation scenario(i.e. Non: $f(z) = z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationFunc(x, ActFun, l):\n",
    "    if ActFun[l] == 'sigmoid':\n",
    "        f = 1 / (1 + np.exp(-x))\n",
    "    if ActFun[l] == 'ReLU':\n",
    "        if x.shape[0] == 1:\n",
    "            f = x\n",
    "            if y < 0:\n",
    "                f = np.array([0])\n",
    "        else:\n",
    "            f = x\n",
    "            f[x < 0] = 0\n",
    "    if ActFun[l] == 'tanh':\n",
    "        f = np.tanh(x)\n",
    "    if ActFun[l] == 'Non':\n",
    "        f = x\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward function: ann()\n",
    "Now we'll come to the feedforward function **<span style=\"color:blue\">ann(x, W, b, ActFun)</span>** in the ANN. Feed in an input, the **ann()** function will provide you the output.\n",
    "\n",
    "* The **structure** of ANN hides in the structure of ANN parameters(i.e. **W** and **b**). User can choose activation function for each layer through **ActFun**. \n",
    "\n",
    "* **W** is a python list, each element in which is a numpy array holding the connection weights of the respective layer. The following example shows the **W** and **b** in the three layer demo ANN mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 layers in the ANN, including 1 hidden layers.\n",
      "The number of inputs is:3\n",
      "The number of hidden layer neurons is:3\n",
      "The number of outputs is:1\n"
     ]
    }
   ],
   "source": [
    "w1 = np.array([[1,1,1], [2,2,2], [3,3,3]]) #weights between input layer and hidden layer\n",
    "w2 = np.array([[1], [1], [1]]) #weights between hidden layer and output layer\n",
    "W = [w1, w2]\n",
    "print('There are %d layers in the ANN, including %d hidden layers.'%(len(W), (len(W) - 1)))\n",
    "print('The number of inputs is:%d'%(w1.shape[0]))\n",
    "print('The number of hidden layer neurons is:%d'%(w1.shape[1]))\n",
    "print('The number of outputs is:%d'%(w2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b** is also a python list, each element in which is a numpy array holding the bias of the respective layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 layers in the ANN, including 2 hidden layers.\n",
      "The number of hidden layer neurons is:3\n",
      "The number of outputs is:1\n"
     ]
    }
   ],
   "source": [
    "b1 = np.array([1,1,1])\n",
    "b2 = np.array([0])\n",
    "b = [b1, b2]\n",
    "print('There are %d layers in the ANN, including %d hidden layers.'%(len(b)+1, len(b)))\n",
    "print('The number of hidden layer neurons is:%d'%(b1.shape[0]))\n",
    "print('The number of outputs is:%d'%(b2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **ActFun** is also a python list denoting the respective activation function of each hidden layer and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosing activation function for the hidden layer and output layer in the three layer ann\n",
    "ActFun = ['sigmoid', 'sigmoid'] #'tanh', 'ReLU', 'Non' are also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(x, W, b, ActFun):\n",
    "    L = len(W) # number of hidden & output layers\n",
    "    dim = np.ones(L, dtype = 'int') # number of neurons in L hidden&output layers\n",
    "    for l in range(L):\n",
    "        dim[l] = W[l].shape[1]\n",
    "    \n",
    "    Y = [] #outputs of L layers\n",
    "    for l in range(L):\n",
    "        Y.append(np.zeros(dim[l]))\n",
    "    \n",
    "    h = x.copy()\n",
    "    for l in range(L):\n",
    "        z = np.matmul(W[l].T, h) + b[l]  #weighted summation\n",
    "        h = activationFunc(z, ActFun, l)   #activation function\n",
    "        Y[l] = h.copy()\n",
    "    return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward experiment\n",
    "Now we will create a simple three layer ANN as mentioned before, with three inputs and one output. We randomly chosing some parameters for the ANN and feed in an input, the ann() function will give us not only the final output from the output layer, but also outputs from hidden layers(in this demonstration, there is only one hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the hidden layer is:\n",
      "[3.6 1.5 2.5]\n",
      "The output of the artificial neural network is:\n",
      "[0.94158544]\n"
     ]
    }
   ],
   "source": [
    "#weights between layer 0(input) and layer 1(hidden)\n",
    "w1 = np.array([[0.3,0.3,0.3],[0.5,0.2,0.4],[0.6,0.1,0.3]])\n",
    "\n",
    "#weights between layer 1(hidden)\n",
    "w2 = np.array([[0.3], [0.3], [0.3]])\n",
    "\n",
    "#bias\n",
    "b1 = np.array([0.5,0.5,0.5])\n",
    "\n",
    "b2 = np.array([0.5])\n",
    "\n",
    "W = [w1, w2]\n",
    "b = [b1, b2]\n",
    "ActFun = ['Non', 'sigmoid'] #select activation functions for the hidden layer and output layer\n",
    "x = np.array([1,2,3]) #the input of the ann\n",
    "Y = ann(x, W, b, ActFun) # ann feedforward\n",
    "\n",
    "print('Output of the hidden layer is:')\n",
    "print(Y[0])\n",
    "print('The output of the artificial neural network is:')\n",
    "print(Y[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What on earth is labeled data?\n",
    "In order to train the neural network to fit the input and output, we need to feed in the input and the output we want simultaneously. In the regression scenario below, we need to input both the independent variables and the function values in the neural network. The function value <span style=\"color:blue\">$y$</span> with respect to each independent variable <span style=\"color:blue\">$x$</span> is the label of <span style=\"color:blue\">$x$</span>. And we call <span style=\"color:blue\">$(x, y)$</span>(e.g. <span style=\"color:blue\">$(1,3.6), (3,14.5), \\dots$</span>) labeled observations (or entries).\n",
    "<img src=\"images/linear_reg.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in image classifications. The classification(what's in that picture e.g. cat, dog,...) corresponding to each picture is the label of that picture.\n",
    "<img src=\"images/image_labeling.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "In this case, (img1, airplane), (img2, dog), (img3, cat),... are the labeled inputs to train a image classification artificial neural networks.\n",
    "\n",
    "In fact, the label is the thing that we hope the artificial neural network would automatically derive when we put an input in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost function\n",
    "Now, we have the input to an ANN and its corresponding output we desire simultaneously. We want to know how close the output of the artificial neural network is to the output we actually desire. And we want to know what is contributing to the distance between them. So we introduce the cost function to describe the distance between the ANN output and the desired one. For an output vector <span style=\"color:blue\">$\\boldsymbol{y}_\\_ \\in \\mathbb{R}^{d_o}$</span> with respect to the input vector <span style=\"color:blue\">$\\boldsymbol{x} \\in \\mathbb{R}^{d_i}$</span> which has the label <span style=\"color:red\">$\\boldsymbol{y} \\in \\mathbb{R}$</span>, the cost function can be demonstated as follows:\n",
    "\n",
    "$$J(\\boldsymbol{w},\\boldsymbol{b},\\boldsymbol{x},\\boldsymbol{y})=\\frac{1}{2}||\\boldsymbol{y}_\\_-\\boldsymbol{\\color{red}y}||_2^2=\\frac{1}{2}||\\boldsymbol{y}_\\_-\\color{red}{h(}\\boldsymbol{\\color{red}x}\\color{red})||_2^2$$\n",
    "\n",
    "Where <span style=\"color:blue\">$h(\\bullet)$</span> denotes the feed-forward process in the artificial neural network. It should be noticed that the loss function in terms of the 2-norm is not the only form of the loss function.\n",
    "\n",
    "In the simple scalar output of the illustrated three layer ANN mentioned before, the cost function is like:\n",
    "\n",
    "$$J(\\boldsymbol{w},\\boldsymbol{b},\\boldsymbol{x},y) = \\frac{1}{2}(y_\\_-\\color{red}y)^2 = \\frac{1}{2}(y_\\_-\\color{red}{h(}\\boldsymbol{\\color{red}x}\\color{red}))^2$$\n",
    "\n",
    "Where $\\color{blue}{y_\\_}$ and $\\color{blue}y$ are scalars and $\\boldsymbol{\\color{blue}x}$ is the input vector of length 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Now, we have the cost function which denotes the distance between the output of the ANN and the output we want. The next thing we want to do is to calibrate the parameters in the artificial neural network so that the distance would become smaller and smaller. So firstly, we need to figure out how the parameters in the ANN(<span style=\"color:blue\">$\\boldsymbol{W},\\boldsymbol{b}$</span>) affect the cost function. And we manage this by the so-called **<span style=\"color:blue\"> backpropagation</span>** process which is based on the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation in the output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look back to the cost function in the three layer ANN mensioned before. The label <span style=\"color:blue\">$y_\\_$</span> is a constant. Hmmm, so the only thing that would affect the cost function is the output <span style=\"color:blue\">$y = f(\\boldsymbol{x})$</span>. So first, I will find a way to quantify how the output <span style=\"color:blue\">$y$</span> contributes to the cost function. Yes, you guess it right, I will calculate the **<span style=\"color:blue\">Derivative</span>** of <span style=\"color:blue\">$J$</span> with respect to <span style=\"color:blue\">$y$</span>.\n",
    "\n",
    "$$\\frac{dJ}{dy} = y - y_\\_$$\n",
    "\n",
    "Now we consider, what contributes to the output <span style=\"color:blue\">$y$</span>. We will look back to the detailed structure of the output layer neuron, this time I will add an intermediate variable after the summation operation.\n",
    "\n",
    "<img src=\"images/output_breakdown.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "$$z_o = w_1^{(2)}h_1 + w_2^{(2)}h_2 + w_3^{(2)}h_3 + b_2$$\n",
    "\n",
    "$$y = f(z_o) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Before outputting, the intermediate variable <span style=\"color:blue\">$z_o$</span> passes through an activation function <span style=\"color:blue\">$f(\\bullet)$</span>. The output <span style=\"color:blue\">$y$</span> would affect the value of the cost function, and the intermediate variable <span style=\"color:blue\">$z_o$</span> would affect the output. So if we want to know how the inermediate variable <span style=\"color:blue\">$z_o$</span>  affcets the value of the cost function, we need to figure how it affects the output <span style=\"color:blue\">$y$</span> first ---- we need to calculate the derivative of the activation function with respect to <span style=\"color:blue\">$z_o$</span>.\n",
    "\n",
    "There are many kind of activation functions: **sigmoid**, **ReLU**, **softmax**, **tanh**, ... For example, if the activation function in the demonstration ANN is **sigmoid** which looks like this:\n",
    "\n",
    "<img src=\"images/sigmoid.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Then we have:\n",
    "\n",
    "$$\\frac{y}{dz_o} = f'(z_o) = f(z_o)(1-f(z_o)) = y(1-y)$$\n",
    "\n",
    "So, according to the chain rule, the derivative of the cost function with respect to the intermediate variable <span style=\"color:blue\">$z_o$</span> is:\n",
    "\n",
    "$$\\frac{dJ}{dz_o} = \\frac{dJ}{dy} \\frac{dy}{dz_o} = y(y - y_\\_)(1-y)% = -y^3+y^2-y_\\_y+y_\\_y^2$$\n",
    "\n",
    "Back to the problem we want to solve: we want to adjust the parameters in the neural network(e.g. <span style=\"color:blue\">$W,b$</span>) to minimize the cost function, so we need to figure out how the parameters will affect the cost function --- we need to calculate the dirivative of costfunction $J$ with respect to those parameters. Now we already have the derivative of $J$ with respect to <span style=\"color:lue\">$z$</span>, to calculate that of $J$ with respect to parameters <span style=\"color:blue\">$w_1^{(2)},w_2^{(2)},w_3^{(2)},b_2$</span> in the output layer, we need to calculate:\n",
    "\n",
    "$$\\frac{\\partial{z}}{\\partial{w_1^{(2)}}} = h_1$$\n",
    "$$\\frac{\\partial{z}}{\\partial{w_2^{(2)}}} = h_2$$\n",
    "$$\\frac{\\partial{z}}{\\partial{w_3^{(2)}}} = h_3$$\n",
    "$$\\frac{\\partial{z}}{\\partial{b_2}} = 1$$\n",
    "\n",
    " According to th chain rule, the derivatives of the cost function $J$ with respect to the parameters in the output layer are calculated as follows:\n",
    "$$\\frac{\\partial{J}}{\\partial{w_1^{(2)}}} = \\frac{dJ}{dy} \\frac{dy}{dz} \\frac{\\partial{z}}{\\partial{w_1^{(2)}}} = h_1 y(y-y_\\_)(1-y)$$\n",
    "$$\\frac{\\partial{J}}{\\partial{w_2^{(2)}}} = \\frac{dJ}{dy} \\frac{dy}{dz} \\frac{\\partial{z}}{\\partial{w_2^{(2)}}} = h_2 y(y-y_\\_)(1-y)$$\n",
    "$$\\frac{\\partial{J}}{\\partial{w_3^{(2)}}} = \\frac{dJ}{dy} \\frac{dy}{dz} \\frac{\\partial{z}}{\\partial{w_3^{(2)}}} = h_3 y(y-y_\\_)(1-y)$$\n",
    "$$\\frac{\\partial{J}}{\\partial{b_2}} = \\frac{dJ}{dy} \\frac{dy}{dz} \\frac{\\partial{z}}{\\partial{b_2}} = y(y-y_\\_)(1-y)$$\n",
    "\n",
    "Now we have the derivatives of the cost function J with respect to the parameters in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagtion in the hidden layer\n",
    "If we look at the structure of the neurons(both hidden and output layer), we'll find that all the parameters(<span style=\"color:blue\">$W,b$</span>) are directly linked to those intermediate variables <span style=\"color:blue\">$z$</span>. \n",
    "\n",
    "<img src=\"images/hidden_output.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "$$z_{h1} = \\color{red}{w_{11}^{(1)}}x_1 + \\color{red}{w_{21}^{(1)}}x_2 + \\color{red}{w_{31}^{(1)}}x_3 + \\color{red}{b_1}$$\n",
    "\n",
    "$$z_o = \\color{red}{w_1^{(2)}}h_1 + \\color{red}{w_2^{(2)}}h_2 + \\color{red}{w_3^{(2)}}h_3 + \\color{red}{b_2}$$\n",
    "\n",
    "Deriving the derivations of those intermediate variables with respect to those parameters is quite easy. The difficult part is deriving the derivations of cost function J with respect to those intermediate variables <span style=\"color:blue\">$z$</span>.\n",
    "\n",
    "Let's leave that example behind for a while, and assume a $(1+L)$ layer ANN with one output layer, $(L-1)$ hiddden layer and one output layer. $l=0$ denotes the input layer and $l=L$ denotes the output layer. For neurons in layer $l={1,2,...,L}$, there exit intermediate variables <span style=\"color:blue\">$z_i^{l}, i=1,2,..,n_l$</span>. <span style=\"color:blue\">$n_l$</span> is the number of neurons in that layer(e.g. <span style=\"color:blue\">$n_L$</span> denotes the number of output neurons). \n",
    "\n",
    "<img src=\"images/n_plus_one_ann.png\" alt=\"Brawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the derivative of J with respect to <span style=\"color:blue\">$z_i^{(L)}, i = 1,2,...,n_L$</span> is quite easy. And we introduce variable <span style=\"color:blue\">$\\delta_i^{(L)}$</span> denoting the derivative of J with respect to <span style=\"color:blue\">$z_i^{(L)}$</span>.\n",
    "\n",
    "$$\\delta_i^{(L)} = \\frac{\\partial{J}}{\\partial{z_i^{(L)}}} = \\frac{\\partial{J}}{\\partial{o_i}} \\frac{do_i}{dz_i^{(L)}}$$\n",
    "\n",
    "Then how do we calculate the derivatives of cost function J with respect to intermediate variavles <span style=\"color:blue\">$z_i^{(l)}$</span> i.e. <span style=\"color:blue\">$\\delta_i^{(l)}$</span>, $i = 1,2,...,n_l, l = 1,2,...,L-1$ in previous layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate variables <span style=\"color:blue\">$z_i^{(L-1)}$</span> in the last hidden layer affect the cost funtion J by affecting the intermediate variables <span style=\"color:blue\">$z_i^{(L)}$</span> in output layer. Each intermediate variable in layer $L-1$ contributes to all $n_L$ variables <span style=\"color:blue\">$z_i^{(L)}$</span> in the next layer(output layer). So the derivative <span style=\"color:blue\">$\\delta_i^{(L-1)}$</span>,$i = 1,2,...,n_{L-1}$ is:\n",
    "\n",
    "$$\\delta_i^{(L-1)} = \\sum_{j=1}^{n_L}\\frac{\\partial{J}}{\\partial{z_j^{(L)}}}\\frac{\\partial{z_j^{(L)}}}{\\partial{z_i^{(L-1)}}} = \\sum_{i=1}^{n_L} \\delta_i^{(L)} w_{ij}^{(L)} f^{(L-1)’}(z_i^{(L-1)})$$\n",
    "\n",
    "Where <span style=\"color:blue\">$w_{ij}^{(L)}$</span> denotes the weight of link from neuron $i$ in layer $L-1$ to neuron $j$ in layer $L$, and $f^{(L-1)’}(z_i^{(L-1)})$ denotes the derivative of the activation function in neuron $i$ in layer $L-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we can recursively computing <span style=\"color:blue\">$\\delta_i^{(l)}$</span>, $i = 1,2,...,n_l, l = 1,2,..., L-1$ by:\n",
    "\n",
    "$$\\delta_i^{(l)} = \\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{J}}{\\partial{z_i^{(l)}}} = \\frac{\\partial{J}}{\\partial{o_i}} \\frac{do_i}{dz_i^{(l)}}, & & l = L\\\\\n",
    "\\sum_{j=1}^{n_{l+1}}\\frac{\\partial{J}}{\\partial{z_j^{(l+1)}}}\\frac{\\partial{z_j^{(l+1)}}}{\\partial{z_i^{(l)}}} = \\sum_{j=1}^{n_{l+1}} \\delta_j^{(l+1)} w_{ij}^{(l+1)} f^{(l)’}(z_i^{(l)}), & & l = 1,2,...,L-1 \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The backpropagation process for intermediate variables <span style=\"color:blue\">$z_i^{(l)}$</span> is illustrated by the graph below:\n",
    "<img src=\"images/backpropagation.png\" alt=\"Brawing\" style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the derivations <span style=\"color:blue\">$\\delta_i^{(l)}$</span>, $i = 1,2,...,n_l , l = 1,2,...,L$, deriving the derivative of cost function J with respect to those parameters(i.e. <span style=\"color:blue\">$W,b$</span>) is quite easy.\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}} = \\frac{\\partial{J}}{\\partial{z_j^{(l)}}} \\frac{\\partial{z_j^{(l)}}}{\\partial{w_{ij}^{(l)}}} = \\delta_j^{(l)} h_i^{(l-1)}, & & & i = 1,2,...,n_{l-1}, j = 1,2,...,n_l, l = 1,2,...,L \\\\\n",
    "\\frac{\\partial{J}}{\\partial{b_i^{(l)}}} = \\frac{\\partial{J}}{\\partial{z_i^{(l)}}} \\frac{\\partial{z_i^{(l)}}}{\\partial{b_i^{(l)}}} = \\delta_i^{(l)}, & & & l = 1,2,...,L\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "We have the derivations of the cost function J with respect to parameters <span style=\"color:blue\">$W,b$</span>, now we want to adjust those parameters to minimize the cost function based on those derivations. And we are going to use a simple but very effective way----**<span style=\"color:blue\">Gradient descent</span>**. In gradient descent, the derivative with respect to those parameters denote the direction in which the parameters would change and how much the change will be. The parameters can be adjusted by the formular below:\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{aligned}\n",
    "w_{ij}^{(l)} = w_{ij}^{(l)} - \\alpha \\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}, & & & i = 1,2,...,n_{l-1}, j = 1,2,...,n_l, l = 1,2,...,L \\\\\n",
    "b_i^{(l)} = b_i^{(l)} - \\alpha \\frac{\\partial{J}}{\\partial{b_i^{(l)}}}, & & & i = 1,2,...,n_{l}, l = 1,2,...,L\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Artificial Neural Network training via Backpropagation in Python\n",
    "Now we are going to train a regressive artificial neural network through python making the simple three layer ANN mentioned before fitting a three variable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import some supporting function library:\n",
    "* numpy: providing data structure and matrix calculation support\n",
    "* time: timing the trianing process\n",
    "* sklearn: randomly selecting the trainning set and the test set\n",
    "* matplotlib: visualization tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation Function of Activation Functions\n",
    "Next, we defined the derivation function of those activationfunctions. But the variale here is not the inputs(i.e. $z$) to the activation function, but the outputs of it(i.e. $f(z)$).(This is convenient to calculate the derivative, you can see derivations in this form in previous sections which introduces activation functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_ActFun(y, ActFun, l):\n",
    "    if ActFun[l] == 'sigmoid':\n",
    "        derivative = np.multiply(y, (1 - y))\n",
    "    if ActFun[l] == 'tanh':\n",
    "        derivative = 1 - y**2\n",
    "    if ActFun[l] == 'ReLU':\n",
    "        derivative = np.ones(y.shape[0])\n",
    "        if y.shape[0] == 1:\n",
    "            if y[0] > 0 or y[0] ==0:\n",
    "                derivative = np.array([1])\n",
    "            else:\n",
    "                derivative = np.array([0])\n",
    "        else:\n",
    "            derivative[y < 0] = 0\n",
    "    if ActFun[l] == 'Non':\n",
    "        derivative = np.ones(y.shape[0])\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Square error calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SquareError(y, y_):\n",
    "    return np.mean((y - y_)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absoute percentage error calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolutePercentageError(y, y_):\n",
    "    return np.mean(np.abs((y - y_)/y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Training Function：bp_train_ann (Backpropagation)\n",
    "The training funciton **<span style=\"color:blue\">bp_train_ann()</span>** uses labeled data **(trianSetX, trainSetY)** from training set to train the neural network and uses the test set to test the trained ANN. \n",
    "\n",
    "* The structure of ANN is also hidden in the initiated parameters **iniW** and **inib** which have the same structure as the **W** and **b** in the feedforward function do.\n",
    "\n",
    "* Variable **alpha** sets the learning rate of the gradient descent process. And variable **maxiter** sets the number of iterations.\n",
    "\n",
    "* The activation function we choose for each layer(hidden & output) is stored in variable **ActFun** which also has the same structure as the feedforward function.\n",
    "\n",
    "* The variable **verify** indicates whether we will test the ANN every iteration and output a list of error(MSE & Mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable **trainSetX** and **testSetX** are numpy arrays of size (numberOfEntries, (numberOfInputs))\n",
    "\n",
    "The variable **trainSetY** and **testSetY** are numpy arrays of size (numberOfEntries, (numberOfOutputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:\n",
      "(5, 3)\n",
      "The input number of the ANN is: 3\n",
      "Size of Y:\n",
      "(6,)\n",
      "The output number of the ANN is: 1\n"
     ]
    }
   ],
   "source": [
    "trainSetX = np.array([[1,1,1],[1,2,1],[2,1,2],[1,1,2],[2,2,1]])\n",
    "trainSetY = np.array([1,2,3,4,5,6])\n",
    "\n",
    "if len(trainSetX.shape) == 2: #2D array: more than 1 inputs\n",
    "    numberOfInputs = trainSetX.shape[1]\n",
    "else:\n",
    "    numberOfInputs = 1\n",
    "print('Size of X:')\n",
    "print(trainSetX.shape)\n",
    "print('The input number of the ANN is: %d'%(numberOfInputs))\n",
    "\n",
    "if len(trainSetY.shape) == 2: #2D array: more than 1 outputs\n",
    "    numberOfOutputs = trainSetY.shape[1]\n",
    "else:\n",
    "    numberOfOutputs = 1\n",
    "print('Size of Y:')\n",
    "print(trainSetY.shape)\n",
    "print('The output number of the ANN is: %d'%(numberOfOutputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_train_ann(trainSetX, trainSetY, testSetX, testSetY, iniW, inib, alpha, maxiter, ActFun, verify = True):\n",
    "    startT = time.time()\n",
    "    #----------------Analyse the structure of ANN--------------------------------------------\n",
    "    numTrainX = trainSetX.shape[0] #size of training set\n",
    "    numTestX = testSetX.shape[0]   #size of test set\n",
    "\n",
    "    if len(trainSetX.shape) == 2: #dimension of inpput\n",
    "        xdim = trainSetX.shape[1]\n",
    "    else:\n",
    "        xdim = 1\n",
    "    if len(trainSetY.shape) == 2: #dimension of output\n",
    "        ydim = trainSetY.shape[1]\n",
    "    else:\n",
    "        ydim = 1\n",
    "    \n",
    "    L = len(iniW) #numer of hidden&output layers\n",
    "    dim = np.ones(L, dtype = 'int') # number of neurons in L hidden&output layers\n",
    "    for l in range(L):\n",
    "        dim[l] = iniW[l].shape[1]\n",
    "\n",
    "    #-------------------------Feedforward & Backpropagation-----------------------------------\n",
    "    W = iniW.copy()\n",
    "    b = inib.copy()\n",
    "    for iter in range(maxiter):\n",
    "        if verify:\n",
    "            trainMSE = []\n",
    "            trainMape = []\n",
    "            testMSE = []\n",
    "            testMape = []\n",
    "        for ind in range(numTrainX):\n",
    "            Y = ann(trainSetX[ind], W, b, ActFun) # ann output of L layers\n",
    "            y_ = trainSetY[ind]  # wanted final output\n",
    "            \n",
    "            Delta = [] #derivative of J with respect to intermediate variale z\n",
    "            for l in range(L):\n",
    "                Delta.append(np.zeros(dim[l]))\n",
    "\n",
    "            #Calculating \\delta_i^{(L)} of the Lth layer(output layer)\n",
    "            Delta[L - 1] = np.multiply((Y[-1] - y_), derivative_ActFun(Y[-1], ActFun, -1))  #(dJ/dy_i) * f'(z_i^L)\n",
    "            #Calculating \\delta_i^{(l)}, l = 1,2,...,L-1\n",
    "            for l in range(L-2, -1, -1): # l =L-1,L-2,...,1\n",
    "                Delta[l] = np.multiply(np.matmul(W[l + 1], Delta[l + 1]), derivative_ActFun(Y[l], ActFun, l)) #f'(z_i^l)\\sum_j(w_{ij}^{(l)} * \\delta_j^{(l+1)})\n",
    "            \n",
    "            H = [] #Input and L-1 hidden output\n",
    "            H.append(trainSetX[ind])\n",
    "            for l in range(L - 1):\n",
    "                H.append(Y[l])\n",
    "            \n",
    "            #Renew parameters\n",
    "            for l in range(L):\n",
    "                W[l] = W[l] - alpha * (1/numTrainX) * np.outer(H[l], Delta[l])\n",
    "#                 print(Delta[l])\n",
    "#                 print(np.outer(H[l], Delta[l]))\n",
    "                b[l] = b[l] - alpha * (1/numTrainX) * Delta[l]\n",
    "#                 print(np.sum(Delta[l]))\n",
    "#                 print()\n",
    "        \n",
    "        #--------------------------Verification-------------------------------------\n",
    "        if verify:\n",
    "            #Calculating Training set MSE/Mape\n",
    "            totalSE = 0\n",
    "            totalApe = 0\n",
    "            for ind in range(numTrainX):\n",
    "                Y = ann(trainSetX[ind], W, b, ActFun) # ann output of L layers\n",
    "                y_ = trainSetY[ind]  # wanted final output\n",
    "                totalSE += SquareError(Y[-1], y_)\n",
    "                totalApe += absolutePercentageError(Y[-1], y_)\n",
    "            trainMSE.append(totalSE/numTrainX)\n",
    "            trainMape.append(totalApe/numTrainX)\n",
    "            \n",
    "            #Calculating Test set MSE/Mape\n",
    "            totalSE = 0\n",
    "            totalApe = 0\n",
    "            for ind in range(numTestX):\n",
    "                Y = ann(testSetX[ind], W, b, ActFun) # ann output of L layers\n",
    "                y_ = testSetY[ind]  # wanted final output\n",
    "                totalSE += SquareError(Y[-1], y_)\n",
    "                totalApe += absolutePercentageError(Y[-1], y_)\n",
    "            testMSE.append(totalSE/numTestX)\n",
    "            testMape.append(totalApe/numTestX)\n",
    "            \n",
    "            if (iter+1) % 100 == 0:\n",
    "                print('%d iteration finished!'%(iter+1))\n",
    "                print('Training set MSE = %f, Mape = %f'%(trainMSE[-1], trainMape[-1]))\n",
    "                print('Test set MSE = %f, Mape = %f'%(testMSE[-1], testMape[-1]))\n",
    "                print()\n",
    "        else:\n",
    "            if (iter+1) % 100 == 0:\n",
    "                print('%d iteration finished!'%(iter+1))\n",
    "                print()\n",
    "    \n",
    "    #-------------------------Finishing-----------------------------------------------\n",
    "    stopT = time.time()\n",
    "    runtime = stopT - startT\n",
    "    print('All %d iterations finished! %f s consumed.'%(maxiter, runtime))\n",
    "    if verify:\n",
    "        print('Training set MSE = %f, Mape = %f'%(trainMSE[-1], trainMape[-1]))\n",
    "        print('Test set MSE = %f, Mape = %f'%(testMSE[-1], testMape[-1]))\n",
    "    else:\n",
    "        totalSE = 0\n",
    "        totalApe = 0        \n",
    "        for ind in range(numTrainX):\n",
    "            Y = ann(trainSetX[ind], W, b, ActFun) # ann output of L layers\n",
    "            y_ = trainSetY[ind]  # wanted final output\n",
    "            totalSE += SquareError(Y[-1], y_)\n",
    "            totalApe += absolutePercentageError(Y[-1], y_)\n",
    "        trainMSE = totalSE/numTrainX\n",
    "        trainMape = totalApe/numTrainX\n",
    "\n",
    "        #Calculating Test set MSE/Mape\n",
    "        totalSE = 0\n",
    "        totalApe = 0\n",
    "        for ind in range(numTestX):\n",
    "            Y = ann(testSetX[ind], W, b, ActFun) # ann output of L layers\n",
    "            y_ = testSetY[ind]  # wanted final output\n",
    "            totalSE += SquareError(Y[-1], y_)\n",
    "            totalApe += absolutePercentageError(Y[-1], y_)\n",
    "        testMSE = totalSE/numTestX\n",
    "        testMape = totalApe/numTestX\n",
    "        print('Training set MSE = %f, Mape = %f'%(trainMSE, trainMape))\n",
    "        print('Test set MSE = %f, Mape = %f'%(testMSE, testMape))\n",
    "    return W, b, trainMSE, trainMape, testMSE, testMape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN training experiments\n",
    "Here we carry out two artificial neural network training experiments, one is training the ANN to fit a three input function $y(\\boldsymbol{x}) = x_1^3 + x_2^2 + x_3 - 1$, the other is training the ANN to recognize digits from 0-9 in the famous **MNIST** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Training ANN to fit a three-inputs function\n",
    "\n",
    "### Create the three-inputs funciton\n",
    "First, we created a three-input funciton $y(x) = x_1^3 + x_2^2 + x_3 -1$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Feedforward and backpropagation to fit a three input function\n",
    "def test_func(X, noiseScale = 0):\n",
    "    if len(X.shape) == 2: #dim(x)==2\n",
    "        y = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = X[i][0]**3 + X[i][1] + X[i][2] - 1 + np.random.normal(0, noiseScale)\n",
    "    else: #dim(x)==1\n",
    "        y = X[0]**2 + X[1] + X[2] - 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a four layer ANN\n",
    "Then we create a fully connected four layer ANN with three inputs and one output. We use function **test_func()** above to generate 1000 noisy points and separate them into training set and test set. \n",
    "\n",
    "Using function **bp_train_ann** to train the four layer ANN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 iteration finished!\n",
      "Training set MSE = 0.244882, Mape = 3.165098\n",
      "Test set MSE = 0.243808, Mape = 2.974797\n",
      "\n",
      "200 iteration finished!\n",
      "Training set MSE = 0.193660, Mape = 3.147141\n",
      "Test set MSE = 0.194121, Mape = 2.988209\n",
      "\n",
      "300 iteration finished!\n",
      "Training set MSE = 0.060529, Mape = 0.844434\n",
      "Test set MSE = 0.064241, Mape = 0.966571\n",
      "\n",
      "400 iteration finished!\n",
      "Training set MSE = 0.059760, Mape = 0.766565\n",
      "Test set MSE = 0.063562, Mape = 0.877633\n",
      "\n",
      "500 iteration finished!\n",
      "Training set MSE = 0.059300, Mape = 0.778403\n",
      "Test set MSE = 0.063006, Mape = 0.883150\n",
      "\n",
      "600 iteration finished!\n",
      "Training set MSE = 0.058808, Mape = 0.795464\n",
      "Test set MSE = 0.062390, Mape = 0.892994\n",
      "\n",
      "700 iteration finished!\n",
      "Training set MSE = 0.058309, Mape = 0.803145\n",
      "Test set MSE = 0.061763, Mape = 0.892862\n",
      "\n",
      "800 iteration finished!\n",
      "Training set MSE = 0.057744, Mape = 0.803592\n",
      "Test set MSE = 0.061017, Mape = 0.885177\n",
      "\n",
      "900 iteration finished!\n",
      "Training set MSE = 0.056989, Mape = 0.793528\n",
      "Test set MSE = 0.059951, Mape = 0.865950\n",
      "\n",
      "1000 iteration finished!\n",
      "Training set MSE = 0.055926, Mape = 0.770653\n",
      "Test set MSE = 0.058372, Mape = 0.831065\n",
      "\n",
      "1100 iteration finished!\n",
      "Training set MSE = 0.054594, Mape = 0.738201\n",
      "Test set MSE = 0.056393, Mape = 0.783126\n",
      "\n",
      "1200 iteration finished!\n",
      "Training set MSE = 0.053300, Mape = 0.703112\n",
      "Test set MSE = 0.054507, Mape = 0.733285\n",
      "\n",
      "1300 iteration finished!\n",
      "Training set MSE = 0.052455, Mape = 0.676012\n",
      "Test set MSE = 0.053270, Mape = 0.696890\n",
      "\n",
      "1400 iteration finished!\n",
      "Training set MSE = 0.052088, Mape = 0.661521\n",
      "Test set MSE = 0.052724, Mape = 0.675376\n",
      "\n",
      "1500 iteration finished!\n",
      "Training set MSE = 0.051945, Mape = 0.654997\n",
      "Test set MSE = 0.052523, Mape = 0.663548\n",
      "\n",
      "1600 iteration finished!\n",
      "Training set MSE = 0.051874, Mape = 0.652642\n",
      "Test set MSE = 0.052438, Mape = 0.656529\n",
      "\n",
      "1700 iteration finished!\n",
      "Training set MSE = 0.051829, Mape = 0.652330\n",
      "Test set MSE = 0.052392, Mape = 0.652566\n",
      "\n",
      "1800 iteration finished!\n",
      "Training set MSE = 0.051794, Mape = 0.652870\n",
      "Test set MSE = 0.052359, Mape = 0.650233\n",
      "\n",
      "1900 iteration finished!\n",
      "Training set MSE = 0.051765, Mape = 0.653780\n",
      "Test set MSE = 0.052332, Mape = 0.648896\n",
      "\n",
      "2000 iteration finished!\n",
      "Training set MSE = 0.051739, Mape = 0.654830\n",
      "Test set MSE = 0.052308, Mape = 0.648187\n",
      "\n",
      "2100 iteration finished!\n",
      "Training set MSE = 0.051715, Mape = 0.655941\n",
      "Test set MSE = 0.052286, Mape = 0.647891\n",
      "\n",
      "2200 iteration finished!\n",
      "Training set MSE = 0.051694, Mape = 0.657051\n",
      "Test set MSE = 0.052266, Mape = 0.647854\n",
      "\n",
      "2300 iteration finished!\n",
      "Training set MSE = 0.051673, Mape = 0.658140\n",
      "Test set MSE = 0.052247, Mape = 0.647994\n",
      "\n",
      "2400 iteration finished!\n",
      "Training set MSE = 0.051654, Mape = 0.659196\n",
      "Test set MSE = 0.052229, Mape = 0.648272\n",
      "\n",
      "2500 iteration finished!\n",
      "Training set MSE = 0.051636, Mape = 0.660208\n",
      "Test set MSE = 0.052211, Mape = 0.648609\n",
      "\n",
      "2600 iteration finished!\n",
      "Training set MSE = 0.051618, Mape = 0.661172\n",
      "Test set MSE = 0.052195, Mape = 0.648990\n",
      "\n",
      "2700 iteration finished!\n",
      "Training set MSE = 0.051601, Mape = 0.662087\n",
      "Test set MSE = 0.052179, Mape = 0.649407\n",
      "\n",
      "2800 iteration finished!\n",
      "Training set MSE = 0.051585, Mape = 0.662950\n",
      "Test set MSE = 0.052164, Mape = 0.649839\n",
      "\n",
      "2900 iteration finished!\n",
      "Training set MSE = 0.051569, Mape = 0.663762\n",
      "Test set MSE = 0.052149, Mape = 0.650267\n",
      "\n",
      "3000 iteration finished!\n",
      "Training set MSE = 0.051553, Mape = 0.664525\n",
      "Test set MSE = 0.052135, Mape = 0.650687\n",
      "\n",
      "3100 iteration finished!\n",
      "Training set MSE = 0.051538, Mape = 0.665237\n",
      "Test set MSE = 0.052121, Mape = 0.651102\n",
      "\n",
      "3200 iteration finished!\n",
      "Training set MSE = 0.051522, Mape = 0.665897\n",
      "Test set MSE = 0.052107, Mape = 0.651504\n",
      "\n",
      "3300 iteration finished!\n",
      "Training set MSE = 0.051507, Mape = 0.666505\n",
      "Test set MSE = 0.052093, Mape = 0.651907\n",
      "\n",
      "3400 iteration finished!\n",
      "Training set MSE = 0.051492, Mape = 0.667066\n",
      "Test set MSE = 0.052079, Mape = 0.652304\n",
      "\n",
      "3500 iteration finished!\n",
      "Training set MSE = 0.051477, Mape = 0.667582\n",
      "Test set MSE = 0.052065, Mape = 0.652693\n",
      "\n",
      "3600 iteration finished!\n",
      "Training set MSE = 0.051461, Mape = 0.668049\n",
      "Test set MSE = 0.052050, Mape = 0.653061\n",
      "\n",
      "3700 iteration finished!\n",
      "Training set MSE = 0.051445, Mape = 0.668458\n",
      "Test set MSE = 0.052036, Mape = 0.653406\n",
      "\n",
      "3800 iteration finished!\n",
      "Training set MSE = 0.051429, Mape = 0.668811\n",
      "Test set MSE = 0.052020, Mape = 0.653730\n",
      "\n",
      "3900 iteration finished!\n",
      "Training set MSE = 0.051413, Mape = 0.669099\n",
      "Test set MSE = 0.052005, Mape = 0.654025\n",
      "\n",
      "4000 iteration finished!\n",
      "Training set MSE = 0.051395, Mape = 0.669314\n",
      "Test set MSE = 0.051988, Mape = 0.654288\n",
      "\n",
      "4100 iteration finished!\n",
      "Training set MSE = 0.051378, Mape = 0.669457\n",
      "Test set MSE = 0.051971, Mape = 0.654516\n",
      "\n",
      "4200 iteration finished!\n",
      "Training set MSE = 0.051359, Mape = 0.669516\n",
      "Test set MSE = 0.051953, Mape = 0.654705\n",
      "\n",
      "4300 iteration finished!\n",
      "Training set MSE = 0.051339, Mape = 0.669481\n",
      "Test set MSE = 0.051934, Mape = 0.654850\n",
      "\n",
      "4400 iteration finished!\n",
      "Training set MSE = 0.051319, Mape = 0.669345\n",
      "Test set MSE = 0.051913, Mape = 0.654946\n",
      "\n",
      "4500 iteration finished!\n",
      "Training set MSE = 0.051297, Mape = 0.669099\n",
      "Test set MSE = 0.051891, Mape = 0.654990\n",
      "\n",
      "4600 iteration finished!\n",
      "Training set MSE = 0.051274, Mape = 0.668741\n",
      "Test set MSE = 0.051867, Mape = 0.654976\n",
      "\n",
      "4700 iteration finished!\n",
      "Training set MSE = 0.051249, Mape = 0.668264\n",
      "Test set MSE = 0.051842, Mape = 0.654899\n",
      "\n",
      "4800 iteration finished!\n",
      "Training set MSE = 0.051223, Mape = 0.667659\n",
      "Test set MSE = 0.051814, Mape = 0.654756\n",
      "\n",
      "4900 iteration finished!\n",
      "Training set MSE = 0.051195, Mape = 0.666915\n",
      "Test set MSE = 0.051785, Mape = 0.654553\n",
      "\n",
      "5000 iteration finished!\n",
      "Training set MSE = 0.051166, Mape = 0.666029\n",
      "Test set MSE = 0.051752, Mape = 0.654276\n",
      "\n",
      "5100 iteration finished!\n",
      "Training set MSE = 0.051134, Mape = 0.664997\n",
      "Test set MSE = 0.051718, Mape = 0.653942\n",
      "\n",
      "5200 iteration finished!\n",
      "Training set MSE = 0.051100, Mape = 0.663824\n",
      "Test set MSE = 0.051681, Mape = 0.653542\n",
      "\n",
      "5300 iteration finished!\n",
      "Training set MSE = 0.051063, Mape = 0.662518\n",
      "Test set MSE = 0.051641, Mape = 0.653073\n",
      "\n",
      "5400 iteration finished!\n",
      "Training set MSE = 0.051025, Mape = 0.661078\n",
      "Test set MSE = 0.051598, Mape = 0.652538\n",
      "\n",
      "5500 iteration finished!\n",
      "Training set MSE = 0.050983, Mape = 0.659524\n",
      "Test set MSE = 0.051552, Mape = 0.651944\n",
      "\n",
      "5600 iteration finished!\n",
      "Training set MSE = 0.050939, Mape = 0.657863\n",
      "Test set MSE = 0.051503, Mape = 0.651306\n",
      "\n",
      "5700 iteration finished!\n",
      "Training set MSE = 0.050893, Mape = 0.656101\n",
      "Test set MSE = 0.051451, Mape = 0.650641\n",
      "\n",
      "5800 iteration finished!\n",
      "Training set MSE = 0.050843, Mape = 0.654268\n",
      "Test set MSE = 0.051396, Mape = 0.649966\n",
      "\n",
      "5900 iteration finished!\n",
      "Training set MSE = 0.050791, Mape = 0.652372\n",
      "Test set MSE = 0.051339, Mape = 0.649275\n",
      "\n",
      "6000 iteration finished!\n",
      "Training set MSE = 0.050736, Mape = 0.650403\n",
      "Test set MSE = 0.051278, Mape = 0.648578\n",
      "\n",
      "6100 iteration finished!\n",
      "Training set MSE = 0.050679, Mape = 0.648365\n",
      "Test set MSE = 0.051216, Mape = 0.647889\n",
      "\n",
      "6200 iteration finished!\n",
      "Training set MSE = 0.050619, Mape = 0.646269\n",
      "Test set MSE = 0.051152, Mape = 0.647228\n",
      "\n",
      "6300 iteration finished!\n",
      "Training set MSE = 0.050558, Mape = 0.644120\n",
      "Test set MSE = 0.051086, Mape = 0.646558\n",
      "\n",
      "6400 iteration finished!\n",
      "Training set MSE = 0.050494, Mape = 0.641900\n",
      "Test set MSE = 0.051019, Mape = 0.645916\n",
      "\n",
      "6500 iteration finished!\n",
      "Training set MSE = 0.050430, Mape = 0.639579\n",
      "Test set MSE = 0.050952, Mape = 0.645228\n",
      "\n",
      "6600 iteration finished!\n",
      "Training set MSE = 0.050365, Mape = 0.637154\n",
      "Test set MSE = 0.050884, Mape = 0.644469\n",
      "\n",
      "6700 iteration finished!\n",
      "Training set MSE = 0.050300, Mape = 0.634592\n",
      "Test set MSE = 0.050817, Mape = 0.643611\n",
      "\n",
      "6800 iteration finished!\n",
      "Training set MSE = 0.050234, Mape = 0.631889\n",
      "Test set MSE = 0.050751, Mape = 0.642636\n",
      "\n",
      "6900 iteration finished!\n",
      "Training set MSE = 0.050170, Mape = 0.629019\n",
      "Test set MSE = 0.050686, Mape = 0.641519\n",
      "\n",
      "7000 iteration finished!\n",
      "Training set MSE = 0.050106, Mape = 0.625991\n",
      "Test set MSE = 0.050623, Mape = 0.640260\n",
      "\n",
      "7100 iteration finished!\n",
      "Training set MSE = 0.050045, Mape = 0.622895\n",
      "Test set MSE = 0.050561, Mape = 0.638845\n",
      "\n",
      "7200 iteration finished!\n",
      "Training set MSE = 0.049984, Mape = 0.619680\n",
      "Test set MSE = 0.050502, Mape = 0.637273\n",
      "\n",
      "7300 iteration finished!\n",
      "Training set MSE = 0.049926, Mape = 0.616336\n",
      "Test set MSE = 0.050445, Mape = 0.635578\n",
      "\n",
      "7400 iteration finished!\n",
      "Training set MSE = 0.049870, Mape = 0.612879\n",
      "Test set MSE = 0.050390, Mape = 0.633758\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 iteration finished!\n",
      "Training set MSE = 0.049816, Mape = 0.609385\n",
      "Test set MSE = 0.050338, Mape = 0.631802\n",
      "\n",
      "7600 iteration finished!\n",
      "Training set MSE = 0.049765, Mape = 0.605850\n",
      "Test set MSE = 0.050288, Mape = 0.629701\n",
      "\n",
      "7700 iteration finished!\n",
      "Training set MSE = 0.049716, Mape = 0.602262\n",
      "Test set MSE = 0.050240, Mape = 0.627499\n",
      "\n",
      "7800 iteration finished!\n",
      "Training set MSE = 0.049669, Mape = 0.598632\n",
      "Test set MSE = 0.050195, Mape = 0.625220\n",
      "\n",
      "7900 iteration finished!\n",
      "Training set MSE = 0.049624, Mape = 0.595008\n",
      "Test set MSE = 0.050152, Mape = 0.622848\n",
      "\n",
      "8000 iteration finished!\n",
      "Training set MSE = 0.049582, Mape = 0.591414\n",
      "Test set MSE = 0.050111, Mape = 0.620428\n",
      "\n",
      "8100 iteration finished!\n",
      "Training set MSE = 0.049542, Mape = 0.587825\n",
      "Test set MSE = 0.050072, Mape = 0.617952\n",
      "\n",
      "8200 iteration finished!\n",
      "Training set MSE = 0.049505, Mape = 0.584259\n",
      "Test set MSE = 0.050035, Mape = 0.615440\n",
      "\n",
      "8300 iteration finished!\n",
      "Training set MSE = 0.049469, Mape = 0.580763\n",
      "Test set MSE = 0.050000, Mape = 0.612944\n",
      "\n",
      "8400 iteration finished!\n",
      "Training set MSE = 0.049435, Mape = 0.577341\n",
      "Test set MSE = 0.049967, Mape = 0.610457\n",
      "\n",
      "8500 iteration finished!\n",
      "Training set MSE = 0.049404, Mape = 0.573978\n",
      "Test set MSE = 0.049936, Mape = 0.607953\n",
      "\n",
      "8600 iteration finished!\n",
      "Training set MSE = 0.049374, Mape = 0.570717\n",
      "Test set MSE = 0.049906, Mape = 0.605454\n",
      "\n",
      "8700 iteration finished!\n",
      "Training set MSE = 0.049346, Mape = 0.567547\n",
      "Test set MSE = 0.049878, Mape = 0.602987\n",
      "\n",
      "8800 iteration finished!\n",
      "Training set MSE = 0.049320, Mape = 0.564450\n",
      "Test set MSE = 0.049852, Mape = 0.600569\n",
      "\n",
      "8900 iteration finished!\n",
      "Training set MSE = 0.049295, Mape = 0.561412\n",
      "Test set MSE = 0.049827, Mape = 0.598226\n",
      "\n",
      "9000 iteration finished!\n",
      "Training set MSE = 0.049271, Mape = 0.558457\n",
      "Test set MSE = 0.049803, Mape = 0.595906\n",
      "\n",
      "9100 iteration finished!\n",
      "Training set MSE = 0.049249, Mape = 0.555572\n",
      "Test set MSE = 0.049781, Mape = 0.593601\n",
      "\n",
      "9200 iteration finished!\n",
      "Training set MSE = 0.049229, Mape = 0.552777\n",
      "Test set MSE = 0.049760, Mape = 0.591354\n",
      "\n",
      "9300 iteration finished!\n",
      "Training set MSE = 0.049209, Mape = 0.550101\n",
      "Test set MSE = 0.049740, Mape = 0.589174\n",
      "\n",
      "9400 iteration finished!\n",
      "Training set MSE = 0.049191, Mape = 0.547484\n",
      "Test set MSE = 0.049721, Mape = 0.587059\n",
      "\n",
      "9500 iteration finished!\n",
      "Training set MSE = 0.049174, Mape = 0.544931\n",
      "Test set MSE = 0.049704, Mape = 0.584969\n",
      "\n",
      "9600 iteration finished!\n",
      "Training set MSE = 0.049158, Mape = 0.542441\n",
      "Test set MSE = 0.049687, Mape = 0.582909\n",
      "\n",
      "9700 iteration finished!\n",
      "Training set MSE = 0.049143, Mape = 0.540020\n",
      "Test set MSE = 0.049671, Mape = 0.580876\n",
      "\n",
      "9800 iteration finished!\n",
      "Training set MSE = 0.049129, Mape = 0.537674\n",
      "Test set MSE = 0.049656, Mape = 0.578879\n",
      "\n",
      "9900 iteration finished!\n",
      "Training set MSE = 0.049115, Mape = 0.535400\n",
      "Test set MSE = 0.049642, Mape = 0.576911\n",
      "\n",
      "10000 iteration finished!\n",
      "Training set MSE = 0.049102, Mape = 0.533199\n",
      "Test set MSE = 0.049629, Mape = 0.574973\n",
      "\n",
      "All 10000 iterations finished! 892.386873 s consumed.\n",
      "Training set MSE = 0.049102, Mape = 0.533199\n",
      "Test set MSE = 0.049629, Mape = 0.574973\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "TrainPercent = 0.7\n",
    "\n",
    "X = np.random.rand(N, 3)\n",
    "y = test_func(X, noiseScale = 0.02)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "#Train a three layer ANN(1 hidden layer) fitting those 4-dimensional points.\n",
    "#weights between layer 0(input) and layer 1(hidden):3 inputs, 3 hidden layer neurons\n",
    "w1 = np.random.rand(3, 3)\n",
    "#weights between layer 1(hidden) # one output\n",
    "w2 = np.random.rand(3, 3)\n",
    "w3 = np.random.rand(3, 1)\n",
    "\n",
    "b1 = np.random.rand(3)\n",
    "b2 = np.random.rand(3)\n",
    "b3 = np.random.rand(1)\n",
    "#bias\n",
    "inib = [b1, b2, b3]\n",
    "#learning rate\n",
    "alpha = 10\n",
    "maxiter = 10000\n",
    "iniW = [w1, w2, w3]\n",
    "ActFun = ['sigmoid', 'sigmoid', 'sigmoid'] #select activation functions for the hidden layer and output layer\n",
    "W, b, trainMSE, trainMape, testMSE, testMape = bp_train_ann(X_train, \n",
    "                                                            y_train, X_test, y_test,\n",
    "                                                            iniW, inib, alpha, maxiter, ActFun, verify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value of f(x) is:\n",
      "[-0.32617008  0.15876826 -0.67619845 -0.0476035  -0.63291076  0.12500175\n",
      "  0.69925925 -0.14159034  1.16440372  0.30354123]\n",
      "ANN calculated f(x) is:\n",
      "[2.25120876e-04 1.45920398e-01 1.30728419e-05 1.89687210e-02\n",
      " 1.89295602e-05 1.19498343e-01 6.95107258e-01 7.77164186e-03\n",
      " 9.90851475e-01 2.98115588e-01]\n"
     ]
    }
   ],
   "source": [
    "test_x = np.random.rand(10,3)\n",
    "y_ = test_func(test_x)               # real value of the function\n",
    "y = np.zeros(test_x.shape[0])\n",
    "for i in range(test_x.shape[0]):\n",
    "    Y = ann(test_x[i], W, b, ActFun)\n",
    "    y[i] = Y[-1]                     #output value of the ANN\n",
    "\n",
    "print('Real value of f(x) is:')\n",
    "print(y_)\n",
    "print('ANN calculated f(x) is:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_x[:, 0], y)\n",
    "plt.scatter(test_x[:, 0], y_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_x[:, 0], y)\n",
    "plt.scatter(test_x[:, 0], y_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_x[:, 0], y)\n",
    "plt.scatter(test_x[:, 0], y_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: digits recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained an artificial neural networks to recognise digits from 0 to 9 using the famous **MNIST** dataset. The **MNIST** dataset contains 1797 digits images of size $8 \\times 8$ and their corresponding labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST dataset\n",
    "First, we load the MNIST dataset from sklearn and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()   #load the dataset from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable **digits** has three subsets: \n",
    "* **images**: which are $8\\times8$ digits images.\n",
    "* **data**: which are vectors(1D arrays) of length 64(data[i] = reshape(images[i], (64)))\n",
    "* **target**: which are the corresponding numbers in those images(datas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of images is:\n",
      "(1797, 8, 8)\n",
      "Size of datas is:\n",
      "(1797, 64)\n",
      "Size of targets is:\n",
      "(1797,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAMO0lEQVR4nO3d+4tc9RnH8c/HTeJ6iaY1VsWIl1IDotRoaisp0iYqsYottNQIWiotKaUVRalosVj/AbE/FEG8VDAq3gLF1htVEcGqSYzXxGJEcb2t4iUxrUnWPP1hTkqabt2zyfl+dzLP+wVDZndn53lmN5/5npk95zyOCAEYbHtMdQMAyiPoQAIEHUiAoAMJEHQgAYIOJNAXQbe92PYrtl+1fXnhWjfZHrX9Ysk629U7zPajttfYfsn2RYXrDdt+2vZzTb2rS9Zrag7Zftb2faVrNfVet/2C7dW2VxSuNcv23bbXNr/DkwvWmts8pm2X9bYv7uTOI2JKL5KGJK2TdJSkGZKek3RMwXqnSDpB0ouVHt8hkk5ors+U9I/Cj8+S9m2uT5f0lKRvFX6Ml0i6TdJ9lX6mr0uaXanWLZJ+3lyfIWlWpbpDkt6VdHgX99cPK/pJkl6NiNciYrOkOyR9v1SxiHhc0oel7n+ceu9ExKrm+gZJayQdWrBeRMSnzYfTm0uxvaJsz5F0pqQbStWYKrb3U29huFGSImJzRHxcqfwiSesi4o0u7qwfgn6opDe3+3hEBYMwlWwfIWmeeqtsyTpDtldLGpX0cESUrHetpMskbS1YY0ch6SHbK20vLVjnKEnvS7q5eWlyg+19Ctbb3hJJt3d1Z/0QdI/zuYHbL9f2vpLukXRxRKwvWSsiPo+I4yXNkXSS7WNL1LF9lqTRiFhZ4v6/wIKIOEHSGZJ+ZfuUQnWmqfcy77qImCdpo6Si7yFJku0Zks6WdFdX99kPQR+RdNh2H8+R9PYU9VKE7enqhXxZRNxbq26zmfmYpMWFSiyQdLbt19V7ybXQ9q2Fav1HRLzd/Dsqabl6L/9KGJE0st0W0d3qBb+0MyStioj3urrDfgj6M5K+ZvvI5plsiaQ/T3FPnbFt9V7jrYmIayrUO9D2rOb6XpJOlbS2RK2IuCIi5kTEEer93h6JiPNK1NrG9j62Z267Lul0SUX+ghIR70p60/bc5lOLJL1cotYOzlWHm+1Sb9NkSkXEmO1fS3pQvXcab4qIl0rVs327pO9Imm17RNJVEXFjqXrqrXrnS3qhed0sSb+NiL8WqneIpFtsD6n3RH5nRFT5s1clB0la3nv+1DRJt0XEAwXrXShpWbMIvSbpgoK1ZHtvSadJ+kWn99u8lQ9ggPXDpjuAwgg6kABBBxIg6EACBB1IoK+CXnh3ximrRT3qTXW9vgq6pJo/zKq/OOpRbyrr9VvQARRQZIeZGd4zhjX5g3y2aJOma8/O++m61tjsyT+2sc82atrwzh34dPDBkz+qdsOHY5r55Z3b8fGtjbMm/T1bN2zUHjN37vENj2yZ9Pds3vovzdhjr52qF1vGJv09Nf9v7kq9z7RRm2PT/xwoVmQX2GHto296UYm77gsf/LDYSUbG9ZtL76ha73cri50OYFxHX/JO1Xpj73Z2rEjfeSr+Nu7n2XQHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAq6DXHJkEoHsTBr05yeAf1TsF7TGSzrV9TOnGAHSnzYpedWQSgO61CXqakUnAoGpzUEurkUnNgfJLJWlYe+9iWwC61GZFbzUyKSKuj4j5ETG/5uF8ACbWJugDPTIJyGDCTffaI5MAdK/ViSeaOWGlZoUBKIw944AECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJFBkUsugqz05ZcnMj6rWu3bWp1Xr/WXVg1Xrnfj7X1atN/v6J6vWGw8rOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxJoM5LpJtujtl+s0RCA7rVZ0f8kaXHhPgAUNGHQI+JxSR9W6AVAIbxGBxLo7DBVZq8B/auzFZ3Za0D/YtMdSKDNn9dul/SkpLm2R2z/rHxbALrUZsjiuTUaAVAOm+5AAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxIYiNlrYwtPrFpvyczVVeudsXhJ1Xr7P7+2ar0fP7Goar0P531etd7sqtXGx4oOJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kQNCBBNqcHPIw24/aXmP7JdsX1WgMQHfa7Os+JunSiFhle6aklbYfjoiXC/cGoCNtZq+9ExGrmusbJK2RdGjpxgB0Z1Kv0W0fIWmepKdKNAOgjNaHqdreV9I9ki6OiPXjfJ3Za0CfarWi256uXsiXRcS9492G2WtA/2rzrrsl3ShpTURcU74lAF1rs6IvkHS+pIW2VzeX7xXuC0CH2sxee0KSK/QCoBD2jAMSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kMBAzF777IC6D+PK0eOq1ttaeRZabc+88NWpbmHgsaIDCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQggTZngR22/bTt55rZa1fXaAxAd9rsJL5J0sKI+LQ5v/sTtu+PiL8X7g1AR9qcBTYkfdp8OL25RMmmAHSr7aSWIdurJY1KejgimL0G7EZaBT0iPo+I4yXNkXSS7WN3vI3tpbZX2F6xRZu67hPALpjUu+4R8bGkxyQtHudrzF4D+lSbd90PtD2rub6XpFMlDfaZEIAB0+Zd90Mk3WJ7SL0nhjsj4r6ybQHoUpt33Z+XNK9CLwAKYc84IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJDMbstS/Vfb5a9uTJVesdraer1qtt2v6bq9Yb+2RG1Xr9gBUdSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IgKADCbQOejPE4VnbnBgS2M1MZkW/SNKaUo0AKKftSKY5ks6UdEPZdgCU0HZFv1bSZZK2FuwFQCFtJrWcJWk0IlZOcDtmrwF9qs2KvkDS2bZfl3SHpIW2b93xRsxeA/rXhEGPiCsiYk5EHCFpiaRHIuK84p0B6Ax/RwcSmNSppCLiMfXGJgPYjbCiAwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IYCBmrw1/VPegum8ct65qvU+qVpOmHXxQ1XrnHPOFx0t17s77v121Xj9gRQcSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACrXaBbU71vEHS55LGImJ+yaYAdGsy+7p/NyI+KNYJgGLYdAcSaBv0kPSQ7ZW2l5ZsCED32m66L4iIt21/RdLDttdGxOPb36B5AlgqScPau+M2AeyKVit6RLzd/Dsqabmkk8a5DbPXgD7VZprqPrZnbrsu6XRJL5ZuDEB32my6HyRpue1tt78tIh4o2hWATk0Y9Ih4TdLXK/QCoBD+vAYkQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IIGBmL223yt1p5NdNee+qvV+svSSqvWm/+D9qvVqO/KKJ6e6hepY0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAq6DbnmX7bttrba+xfXLpxgB0p+2+7n+Q9EBE/Mj2DIkJDcDuZMKg295P0imSfipJEbFZ0uaybQHoUptN96MkvS/pZtvP2r6hGeTwX2wvtb3C9oot2tR5owB2XpugT5N0gqTrImKepI2SLt/xRoxkAvpXm6CPSBqJiKeaj+9WL/gAdhMTBj0i3pX0pu25zacWSXq5aFcAOtX2XfcLJS1r3nF/TdIF5VoC0LVWQY+I1ZLmF+4FQCHsGQckQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IIGBmL229fm1Veudc92lVetdeentVetdu25R1XrPHD9UtV5GrOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACEwbd9lzbq7e7rLd9cY3mAHRjwl1gI+IVScdLku0hSW9JWl64LwAdmuym+yJJ6yLijRLNAChjskFfIqnuERYAdlnroDfndD9b0l3/5+vMXgP61GRW9DMkrYqI98b7IrPXgP41maCfKzbbgd1Sq6Db3lvSaZLuLdsOgBLajmT6p6QDCvcCoBD2jAMSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxJwRHR/p/b7knbmmPXZkj7ouJ1+qEU96tWqd3hEHLjjJ4sEfWfZXhER8wetFvWoN9X12HQHEiDoQAL9FvTrB7QW9ag3pfX66jU6gDL6bUUHUABBBxIg6EACBB1IgKADCfwb4JuwCTaYQkUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = digits.images\n",
    "datas = digits.data\n",
    "targets = digits.target\n",
    "\n",
    "print('Size of images is:')\n",
    "print(images.shape)\n",
    "print('Size of datas is:')\n",
    "print(datas.shape)\n",
    "print('Size of targets is:')\n",
    "print(targets.shape)\n",
    "\n",
    "plt.matshow(images[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the images(datas) are integers ranging from 0 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(datas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to standardize those inputs to $(-1, 1)$ first so that it will help the training of ANN convergent. We will use the function **StandardScaler** from sklearn to help us scaling the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.33501649 -0.04308102  0.27407152 -0.66447751]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "InputScaling = StandardScaler()\n",
    "X = InputScaling.fit_transform(datas)\n",
    "print(X[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels in the variable **targets** are 0-9 integers which are the digits in the corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the image above is 0\n"
     ]
    }
   ],
   "source": [
    "print('The label of the image above is %d'%(targets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we do not use 0-9 integers in the ANN as output, instead, we use an one-hot vector of lenth 10 to indicate the corresponding digits. By the word **one-hot** I mean it's a binary vector with only one element of which is 1 and others are 0. For example, I can use vector **[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]** to indicate digit 1, and use **[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]** to indicate digit 3. So I need to write a function to transform those targets into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the image above is transformed to:\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def num2vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "y = num2vect(targets)\n",
    "print('The label of the image above is transformed to:')\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use a function **train_test_split** from sklearn to randomly choose some elements in **datas** and **targets** as traning set and make the remains as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 64)\n",
      "[ 0.         -0.33501649 -1.09493684  0.50949529  0.03544399]\n",
      "(1078, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "print(X_train.shape)\n",
    "print(X_train[0][0:5])\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an ANN\n",
    "We created a three layer artificail neural network to recognize those digits. The ANN has 68 inputs, 30 hidden layer neurons and 10 outputs.\n",
    "\n",
    "The 10 outputs correspond to the 10 elements in the transformed label(one-hot vector).\n",
    "\n",
    "But we shouldn't expect the output of the ANN to also be an on-hot vector. Actually, the outputs we get from the ANN may look like this: **[0.01, 0.1, 0.08, 0.16, 0.75, 0.03, 0.21, 0.02, 0.03, 0.15]**, and we assume the one with the biggest number(closet to 1)  **0.75** indicates the most likely digits the image will be. And in this case, the most likely digit is **4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 iteration finished!\n",
      "\n",
      "200 iteration finished!\n",
      "\n",
      "300 iteration finished!\n",
      "\n",
      "400 iteration finished!\n",
      "\n",
      "500 iteration finished!\n",
      "\n",
      "600 iteration finished!\n",
      "\n",
      "700 iteration finished!\n",
      "\n",
      "800 iteration finished!\n",
      "\n",
      "900 iteration finished!\n",
      "\n",
      "1000 iteration finished!\n",
      "\n",
      "1100 iteration finished!\n",
      "\n",
      "1200 iteration finished!\n",
      "\n",
      "1300 iteration finished!\n",
      "\n",
      "1400 iteration finished!\n",
      "\n",
      "1500 iteration finished!\n",
      "\n",
      "1600 iteration finished!\n",
      "\n",
      "1700 iteration finished!\n",
      "\n",
      "1800 iteration finished!\n",
      "\n",
      "1900 iteration finished!\n",
      "\n",
      "2000 iteration finished!\n",
      "\n",
      "2100 iteration finished!\n",
      "\n",
      "2200 iteration finished!\n",
      "\n",
      "2300 iteration finished!\n",
      "\n",
      "2400 iteration finished!\n",
      "\n",
      "2500 iteration finished!\n",
      "\n",
      "2600 iteration finished!\n",
      "\n",
      "2700 iteration finished!\n",
      "\n",
      "2800 iteration finished!\n",
      "\n",
      "2900 iteration finished!\n",
      "\n",
      "3000 iteration finished!\n",
      "\n",
      "3100 iteration finished!\n",
      "\n",
      "3200 iteration finished!\n",
      "\n",
      "3300 iteration finished!\n",
      "\n",
      "3400 iteration finished!\n",
      "\n",
      "3500 iteration finished!\n",
      "\n",
      "3600 iteration finished!\n",
      "\n",
      "3700 iteration finished!\n",
      "\n",
      "3800 iteration finished!\n",
      "\n",
      "3900 iteration finished!\n",
      "\n",
      "4000 iteration finished!\n",
      "\n",
      "4100 iteration finished!\n",
      "\n",
      "4200 iteration finished!\n",
      "\n",
      "4300 iteration finished!\n",
      "\n",
      "4400 iteration finished!\n",
      "\n",
      "4500 iteration finished!\n",
      "\n",
      "4600 iteration finished!\n",
      "\n",
      "4700 iteration finished!\n",
      "\n",
      "4800 iteration finished!\n",
      "\n",
      "4900 iteration finished!\n",
      "\n",
      "5000 iteration finished!\n",
      "\n",
      "All 5000 iterations finished! 313.084678 s consumed.\n",
      "Training set MSE = 0.012692, Mape = inf\n",
      "Test set MSE = 0.020535, Mape = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.rand(64, 30)\n",
    "#weights between layer 1(hidden) # one output\n",
    "w2 = np.random.rand(30, 10)\n",
    "\n",
    "b1 = np.random.rand(30)\n",
    "b2 = np.random.rand(10)\n",
    "#bias\n",
    "inib = [b1, b2]\n",
    "#learning rate\n",
    "alpha = 0.25\n",
    "maxiter = 5000\n",
    "iniW = [w1, w2]\n",
    "ActFun = ['sigmoid', 'sigmoid'] #select activation functions for the hidden layer and output layer\n",
    "W, b, trainMSE, trainMape, testMSE, testMape = bp_train_ann(X_train, y_train, X_test, y_test,\n",
    "                                       iniW, inib, alpha, maxiter, ActFun, verify = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained a ANN which would provide us a label vector once we feed in a digits image. The next thing to do is transform the label vector into digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digitRec(W, b, X, ActFun):\n",
    "    m = X.shape[0]\n",
    "    if len(X.shape) == 2:\n",
    "        y = np.zeros((m,))\n",
    "        for i in range(m):\n",
    "            Y = ann(X[i, :], W, b, ActFun)\n",
    "            y[i] = np.argmax(Y[-1])\n",
    "    else:\n",
    "        Y = ann(X, W, b, ActFun)\n",
    "        y = np.argmax(Y[-1])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2num(vec):\n",
    "    if len(vec.shape) == 1:\n",
    "        num = np.argmax(vec)\n",
    "    else:\n",
    "        num = np.zeros(vec.shape[0])\n",
    "        for i in range(vec.shape[0]):\n",
    "            num[i] = np.argmax(vec[i])\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the recognition accuracy of the trained ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rcognition accuracy is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.82058414464534"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_rec = digitRec(W, b, X_test, ActFun)\n",
    "print('Rcognition accuracy is:')\n",
    "accuracy_score(vec2num(y_test), y_rec)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are a few recognition demos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real digit is 0, recognized as 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 0\n",
    "pred = digitRec(W, b, X[ind], ActFun)\n",
    "print('Real digit is %d, recognized as %d'%(targets[ind], pred))\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[ind]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real digit is 1, recognized as 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 1\n",
    "pred = digitRec(W, b, X[ind], ActFun)\n",
    "print('Real digit is %d, recognized as %d'%(targets[ind], pred))\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[ind]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real digit is 2, recognized as 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 2\n",
    "pred = digitRec(W, b, X[ind], ActFun)\n",
    "print('Real digit is %d, recognized as %d'%(targets[ind], pred))\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[ind]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real digit is 3, recognized as 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 3\n",
    "pred = digitRec(W, b, X[ind], ActFun)\n",
    "print('Real digit is %d, recognized as %d'%(targets[ind], pred))\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[ind]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
